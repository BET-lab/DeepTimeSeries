

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Layers &#8212; DeepTimeSeries  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/layers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Utilities" href="utilities.html" />
    <link rel="prev" title="Models" href="models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">DeepTimeSeries  documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials/index.html">
    Tutorial
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials/index.html">
    Tutorial
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="core.html">Core Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="chunk.html">Chunk Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="transform.html">Data Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">API Reference</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Layers</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="layers">
<h1>Layers<a class="headerlink" href="#layers" title="Permalink to this heading">#</a></h1>
<p>The layer module provides custom neural network layers for time series models. These layers are utility components used by the forecasting models to handle specific operations needed for time series processing.</p>
<p><strong>Purpose:</strong></p>
<p>These layers are building blocks used internally by the forecasting models. While you typically don’t need to use them directly, understanding them can help when customizing models.</p>
<p><strong>Common Use Cases:</strong></p>
<ul class="simple">
<li><p><strong>Dimension Manipulation</strong>: Adding or permuting dimensions for compatibility</p></li>
<li><p><strong>Causal Convolutions</strong>: Ensuring predictions don’t use future information</p></li>
<li><p><strong>Positional Information</strong>: Adding temporal context to transformer models</p></li>
</ul>
<span class="target" id="module-deep_time_series.layer"></span><dl class="py class">
<dt class="sig sig-object py" id="deep_time_series.layer.LeftPadding1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LeftPadding1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#LeftPadding1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.LeftPadding1D" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="deep_time_series.layer.LeftPadding1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#LeftPadding1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.LeftPadding1D.forward" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="deep_time_series.layer.Permute">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Permute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Permute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.Permute" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="deep_time_series.layer.Permute.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Permute.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.Permute.forward" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="deep_time_series.layer.PositionalEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#PositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.PositionalEncoding" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="deep_time_series.layer.PositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#PositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.PositionalEncoding.forward" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="deep_time_series.layer.Unsqueesze">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Unsqueesze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Unsqueesze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.Unsqueesze" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="deep_time_series.layer.Unsqueesze.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Unsqueesze.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#deep_time_series.layer.Unsqueesze.forward" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<section id="unsqueesze">
<h2>Unsqueesze<a class="headerlink" href="#unsqueesze" title="Permalink to this heading">#</a></h2>
<p>A layer that adds a dimension to the input tensor at the specified position.</p>
<p><strong>Purpose:</strong></p>
<p>Wraps PyTorch’s <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> operation as a layer, useful for adding dimensions in sequential models.</p>
<p><strong>Initialization Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> (int): The position at which to insert the new dimension. Must be between <code class="docutils literal notranslate"><span class="pre">-input.ndim-1</span></code> and <code class="docutils literal notranslate"><span class="pre">input.ndim</span></code> (inclusive).</p></li>
</ul>
<p><strong>Input/Output:</strong></p>
<ul class="simple">
<li><p><strong>Input</strong>: Any tensor with shape <code class="docutils literal notranslate"><span class="pre">(...,)</span></code></p></li>
<li><p><strong>Output</strong>: Tensor with an additional dimension at position <code class="docutils literal notranslate"><span class="pre">dim</span></code>, shape <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">1,</span> <span class="pre">...)</span></code></p></li>
</ul>
<p><strong>When to Use:</strong></p>
<ul class="simple">
<li><p>Need to add a time dimension for sequential processing</p></li>
<li><p>Converting between different tensor shapes in a model</p></li>
<li><p>Making tensors compatible with layers expecting specific dimensions</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deep_time_series.layer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Unsqueesze</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">Unsqueesze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># (batch_size, features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (32, 1, 64) - adds dimension at position 1</span>
</pre></div>
</div>
<p><strong>Note:</strong></p>
<p>This is a simple wrapper around <code class="docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code>. The name “Unsqueesze” is a typo in the original code but kept for backward compatibility.</p>
<dl class="py class">
<dt class="sig sig-object py" id="id0">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Unsqueesze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Unsqueesze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id0" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Unsqueesze.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id1" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="permute">
<h2>Permute<a class="headerlink" href="#permute" title="Permalink to this heading">#</a></h2>
<p>A layer that permutes the dimensions of the input tensor according to the specified order.</p>
<p><strong>Purpose:</strong></p>
<p>Wraps PyTorch’s <code class="docutils literal notranslate"><span class="pre">permute()</span></code> operation as a layer, useful for reordering dimensions.</p>
<p><strong>Initialization Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">*dims</span></code> (int): Variable number of integers specifying the desired dimension order. The number of arguments must match the input tensor’s number of dimensions. Each integer represents the dimension index in the original tensor.</p></li>
</ul>
<p><strong>Input/Output:</strong></p>
<ul class="simple">
<li><p><strong>Input</strong>: Tensor with shape <code class="docutils literal notranslate"><span class="pre">(d0,</span> <span class="pre">d1,</span> <span class="pre">...,</span> <span class="pre">dn)</span></code></p></li>
<li><p><strong>Output</strong>: Tensor with permuted dimensions according to <code class="docutils literal notranslate"><span class="pre">dims</span></code>. If <code class="docutils literal notranslate"><span class="pre">dims</span> <span class="pre">=</span> <span class="pre">(i0,</span> <span class="pre">i1,</span> <span class="pre">...,</span> <span class="pre">in)</span></code>, output shape is <code class="docutils literal notranslate"><span class="pre">(d[i0],</span> <span class="pre">d[i1],</span> <span class="pre">...,</span> <span class="pre">d[in])</span></code></p></li>
</ul>
<p><strong>When to Use:</strong></p>
<ul class="simple">
<li><p>Converting between <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">time,</span> <span class="pre">features)</span></code> and <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">features,</span> <span class="pre">time)</span></code> formats</p></li>
<li><p>Making tensors compatible with layers expecting different dimension orders</p></li>
<li><p>Used in CNN models where convolutions expect <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">C,</span> <span class="pre">L)</span></code> format</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deep_time_series.layer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Permute</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">Permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Swap dimensions 1 and 2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># (batch, time, features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (32, 64, 10) - (batch, features, time)</span>
</pre></div>
</div>
<p><strong>Common Patterns:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Permute(0,</span> <span class="pre">2,</span> <span class="pre">1)</span></code>: Convert <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L,</span> <span class="pre">F)</span></code> to <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">F,</span> <span class="pre">L)</span></code> for Conv1d</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Permute(0,</span> <span class="pre">2,</span> <span class="pre">1)</span></code> then back: Convert back after convolution</p></li>
</ul>
<dl class="py class">
<dt class="sig sig-object py" id="id2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Permute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Permute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id2" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#Permute.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id3" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="leftpadding1d">
<h2>LeftPadding1D<a class="headerlink" href="#leftpadding1d" title="Permalink to this heading">#</a></h2>
<p>A layer that adds left padding to 1D sequences. Useful for causal convolutions in time series models.</p>
<p><strong>Purpose:</strong></p>
<p>Ensures causal (non-leaking) convolutions by padding on the left side. This prevents the convolution from using future information when processing time series data.</p>
<p><strong>Initialization Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">padding_size</span></code> (int): Number of zero-padding elements to add at the beginning of the sequence. Must be non-negative.</p></li>
</ul>
<p><strong>Input/Output:</strong></p>
<ul class="simple">
<li><p><strong>Input</strong>: Tensor with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">n_features)</span></code></p></li>
<li><p><strong>Output</strong>: Tensor with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length</span> <span class="pre">+</span> <span class="pre">padding_size,</span> <span class="pre">n_features)</span></code>. The first <code class="docutils literal notranslate"><span class="pre">padding_size</span></code> time steps are zeros, followed by the original sequence.</p></li>
</ul>
<p><strong>When to Use:</strong></p>
<ul class="simple">
<li><p>Implementing causal convolutions (e.g., in DilatedCNN)</p></li>
<li><p>Ensuring temporal order is preserved</p></li>
<li><p>Preventing information leakage from future to past</p></li>
</ul>
<p><strong>How It Works:</strong></p>
<p>Adds zeros to the left (beginning) of the sequence, shifting the original data to the right. This allows convolutions to process the sequence while maintaining causality. The padding is created on the same device and with the same dtype as the input tensor.</p>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deep_time_series.layer</span><span class="w"> </span><span class="kn">import</span> <span class="n">LeftPadding1D</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">LeftPadding1D</span><span class="p">(</span><span class="n">padding_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># (batch, time, features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (32, 12, 64) - 2 zeros added at the beginning</span>
</pre></div>
</div>
<p><strong>Use in Dilated CNN:</strong></p>
<p>In dilated convolutions, the padding size is calculated as <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span></code> to ensure the output length matches the input length while maintaining causality.</p>
<p><strong>Important:</strong></p>
<ul class="simple">
<li><p>Only pads on the left (beginning)</p></li>
<li><p>Padding values are zeros</p></li>
<li><p>Maintains the same device and dtype as input</p></li>
</ul>
<dl class="py class">
<dt class="sig sig-object py" id="id4">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LeftPadding1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#LeftPadding1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id4" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="id5">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#LeftPadding1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id5" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="positionalencoding">
<h2>PositionalEncoding<a class="headerlink" href="#positionalencoding" title="Permalink to this heading">#</a></h2>
<p>Positional encoding layer for transformer models. Adds positional information to input embeddings.</p>
<p><strong>Purpose:</strong></p>
<p>Since transformers don’t have inherent notion of sequence order, positional encoding adds temporal information to help the model understand the position of each time step.</p>
<p><strong>Initialization Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">d_model</span></code> (int): Embedding dimension. Must match the input tensor’s feature dimension. This determines the size of the positional encoding vectors.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_len</span></code> (int): Maximum sequence length to pre-compute encodings for. The positional encodings are pre-computed up to this length for efficiency. If your sequence is longer, the encoding will be truncated.</p></li>
</ul>
<p><strong>Input/Output:</strong></p>
<ul class="simple">
<li><p><strong>Input</strong>: Tensor with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">d_model)</span></code>. The feature dimension must match <code class="docutils literal notranslate"><span class="pre">d_model</span></code>.</p></li>
<li><p><strong>Output</strong>: Tensor with the same shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">d_model)</span></code>. The positional encoding is added (element-wise) to the input embeddings.</p></li>
</ul>
<p><strong>When to Use:</strong></p>
<ul class="simple">
<li><p>Building transformer-based models (e.g., SingleShotTransformer)</p></li>
<li><p>Need to encode temporal position information</p></li>
<li><p>Working with sequences where order matters</p></li>
</ul>
<p><strong>How It Works:</strong></p>
<p>Uses sinusoidal functions (sine and cosine) with different frequencies to encode position. Each dimension of the encoding corresponds to a different frequency, allowing the model to learn relative positions. The encoding is pre-computed during initialization and stored as a buffer.</p>
<p><strong>Architecture:</strong></p>
<p>Based on the original Transformer paper (Vaswani et al., 2017), modified for batch-first format and without dropout. The encoding uses:</p>
<ul class="simple">
<li><p>Sine functions for even dimensions: <code class="docutils literal notranslate"><span class="pre">sin(pos</span> <span class="pre">/</span> <span class="pre">10000^(2i/d_model))</span></code></p></li>
<li><p>Cosine functions for odd dimensions: <code class="docutils literal notranslate"><span class="pre">cos(pos</span> <span class="pre">/</span> <span class="pre">10000^(2i/d_model))</span></code></p></li>
</ul>
<p>where <code class="docutils literal notranslate"><span class="pre">pos</span></code> is the position and <code class="docutils literal notranslate"><span class="pre">i</span></code> is the dimension index.</p>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deep_time_series.layer</span><span class="w"> </span><span class="kn">import</span> <span class="n">PositionalEncoding</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># (batch, time, d_model)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (32, 50, 128) - positional encoding added</span>
</pre></div>
</div>
<p><strong>Key Properties:</strong></p>
<ul class="simple">
<li><p><strong>Sinusoidal</strong>: Uses sin/cos functions for smooth position encoding</p></li>
<li><p><strong>Fixed</strong>: The encoding is deterministic and not learnable (stored as buffer)</p></li>
<li><p><strong>Relative Positions</strong>: The encoding allows the model to understand relative distances between time steps</p></li>
<li><p><strong>Additive</strong>: The encoding is added (not concatenated) to the input embeddings</p></li>
</ul>
<p><strong>Note:</strong></p>
<p>The encoding is added (not concatenated) to the input embeddings. This allows the model to learn how to combine positional and feature information. The positional encodings are registered as buffers, so they are automatically moved to the correct device when the model is moved.</p>
<dl class="py class">
<dt class="sig sig-object py" id="id6">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#PositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id6" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="id7">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/deep_time_series/layer.html#PositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id7" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Models</p>
      </div>
    </a>
    <a class="right-next"
       href="utilities.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Utilities</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.LeftPadding1D"><code class="docutils literal notranslate"><span class="pre">LeftPadding1D</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.LeftPadding1D.forward"><code class="docutils literal notranslate"><span class="pre">LeftPadding1D.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.Permute"><code class="docutils literal notranslate"><span class="pre">Permute</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.Permute.forward"><code class="docutils literal notranslate"><span class="pre">Permute.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.PositionalEncoding"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.PositionalEncoding.forward"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.Unsqueesze"><code class="docutils literal notranslate"><span class="pre">Unsqueesze</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep_time_series.layer.Unsqueesze.forward"><code class="docutils literal notranslate"><span class="pre">Unsqueesze.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsqueesze">Unsqueesze</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id0"><code class="docutils literal notranslate"><span class="pre">Unsqueesze</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><code class="docutils literal notranslate"><span class="pre">Unsqueesze.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#permute">Permute</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><code class="docutils literal notranslate"><span class="pre">Permute</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><code class="docutils literal notranslate"><span class="pre">Permute.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leftpadding1d">LeftPadding1D</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><code class="docutils literal notranslate"><span class="pre">LeftPadding1D</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><code class="docutils literal notranslate"><span class="pre">LeftPadding1D.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positionalencoding">PositionalEncoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/api/layers.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, betlab and dartwork.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>